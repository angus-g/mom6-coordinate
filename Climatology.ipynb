{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climatology\n",
    "\n",
    "As a first step in coordinate development, we'll work on sections of climatology from [WOA13](https://www.nodc.noaa.gov/OC5/woa13/). Because temperature is given in-situ, we first have to convert to potential temperature. Similar to [convert_WOA13](https://github.com/adcroft/convert_WOA13), we use the Python `gsw` package, which implements TEOS-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from remapping import mom_remapping\n",
    "import gsw\n",
    "from scipy.linalg import solve_banded\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an initial set of input files, we load up THREDDS URLs for monthly averaged climatologies from 2005-2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_format = 'https://data.nodc.noaa.gov/thredds/dodsC/woa/WOA13/DATAv2/{}/netcdf/A5B2/1.00/woa13_A5B2_{}{:02}_01v2.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_urls = [url_format.format('temperature', 't', i+1) for i in range(12)]\n",
    "salt_urls = [url_format.format('salinity', 's', i+1) for i in range(12)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a function that gives us a potential density section from a given month (default January), and range of latitudes and longitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sect(lat, lon, month=0):\n",
    "    salt_data = Dataset(salt_urls[month])\n",
    "    temp_data = Dataset(temp_urls[month])\n",
    "    \n",
    "    depth = salt_data.variables['depth'][:]\n",
    "    # calculate indices for lat/lon\n",
    "    lat_d = salt_data.variables['lat'][:]\n",
    "    lon_d = salt_data.variables['lon'][:]\n",
    "    lat_i = (lat.min() <= lat_d) & (lat_d <= lat.max())\n",
    "    lon_i = (lon.min() <= lon_d) & (lon_d <= lon.max())\n",
    "    \n",
    "    # compute absolute salinity from practical salinity\n",
    "    sp = salt_data.variables['s_an'][0,:,lat_i,lon_i]\n",
    "    print(sp.shape)\n",
    "    sa = gsw.SA_from_SP(sp, depth, lon, lat)\n",
    "    print(sa.shape, sa)\n",
    "    \n",
    "    # compute conservative temperature from in-situ temperature and absolute salinity\n",
    "    ct = gsw.CT_from_t(sa, temp_data.variables['t_an'][0,:,lat_i,lon_i], depth)\n",
    "    \n",
    "    salt_data.close()\n",
    "    temp_data.close()\n",
    "    \n",
    "    # compute potential density wrt surface\n",
    "    return gsw.rho(sa, ct, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sections\n",
    "\n",
    "Although our choice of coordinate should apply globally, we're particularly interested in a few troublesome spots, where there tend to always be problems, such as the Denmark Strait and the Sulu Sea. We may also care about dense overflows off Antarctica.\n",
    "\n",
    "## Denmark Strait\n",
    "Because it's pretty easy to find, we'll look at the Denmark strait from 22.5 to 39.5 degrees West, at 63.5 degrees North."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp = Dataset('data/woa13_A5B2_t01_01v2.nc', 'r')\n",
    "salt = Dataset('data/woa13_A5B2_s01_01v2.nc', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lat = temp.variables['lat'][:]\n",
    "lon = temp.variables['lon'][:]\n",
    "dep = temp.variables['depth'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lon_i = (-39.5 <= lon) & (lon <= -22.5)\n",
    "\n",
    "t_sect = temp.variables['t_an'][0,:,lat==63.5,lon_i].squeeze()\n",
    "s_sect = salt.variables['s_an'][0,:,lat==63.5,lon_i].squeeze()\n",
    "lon_sect = lon[lon_i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TEOS-10, we can convert from practical salinity to absolute salinity, and from in-situ temperature to conservative temperature. From here, we can compute the locally-referenced density, and the potential density referenced to 2000m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sa_sect = np.empty_like(s_sect)\n",
    "ct_sect = np.empty_like(t_sect)\n",
    "rho_sect = np.empty_like(s_sect)\n",
    "rhop_sect = np.empty_like(rho_sect)\n",
    "\n",
    "for i in range(s_sect.shape[1]):\n",
    "    sa_sect[:,i] = gsw.SA_from_SP(s_sect[:,i], dep, -39.5 + i, 63.5)\n",
    "    ct_sect[:,i] = gsw.CT_from_t(sa_sect[:,i], t_sect[:,i], dep)\n",
    "    rho_sect[:,i] = gsw.rho(sa_sect[:,i], ct_sect[:,i], dep)\n",
    "    rhop_sect[:,i] = gsw.rho(sa_sect[:,i], ct_sect[:,i], 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the two density sections. The locally-refenced density shows very linear stratification (this is probably expected?), whereas the potential density shows a much clearer mixed layer (also as expected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = plt.subplot(121)\n",
    "plt.pcolormesh(lon_sect, dep, rho_sect)\n",
    "ax.invert_yaxis()\n",
    "#plt.colorbar()\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "plt.pcolormesh(lon_sect, dep, rhop_sect)\n",
    "ax.invert_yaxis()\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're looking at January, so this should be a Winter mixed layer. It's around 100m deep in the shallower regions, but nearly up to 400m deep in the deeper parts of the middle of the strait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(rhop_sect.shape[1]):\n",
    "    plt.plot(rhop_sect[:,i], dep, '*')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mixed layer depth for each column\n",
    "mld = np.array([50, 50, 100, 200, 200, 200, 200, 200, 200, 200, 300, 300, 350, 350, 300, 100, 100, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give some information between water columns, we want to compute neutral density differences. We can use the thermal expansion and haline contraction coefficients from the mean salinity, temperature and pressure of the water parcels, as well as the local density from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rhop_sect[30,5:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sa_c = (sa_sect[30,5] + sa_sect[30,6]) / 2\n",
    "ct_c = (ct_sect[30,5] + ct_sect[30,6]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rho_c, alpha_c, beta_c = gsw.rho_alpha_beta(sa_c, ct_c, dep[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nd_diff = rho_c * (beta_c * (sa_sect[30,5] - sa_sect[30,6]) - alpha_c * (ct_sect[30,5] - ct_sect[30,6]))\n",
    "print(nd_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to compute a distance (in physical space) by which to shift the parcel in order to flatten out the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = 9.7963 # value of gravity used in gsw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n2 = gsw.Nsquared(sa_sect[29:31,5],\n",
    "                  ct_sect[29:31,5],\n",
    "                  dep[29:31])\n",
    "dz = (g**2 * nd_diff) / (1e4 * n2[0])\n",
    "print(dz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Generation\n",
    "\n",
    "We can take our initial grid (which is set by the depths of observations, some of which are masked) and convert that to a representative model grid with millimetre-thick layers instead of completely missing layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# depths of values with local mask\n",
    "gr_dep = np.ma.array(np.tile(dep.reshape(-1, 1), (1, t_sect.shape[1])), mask=t_sect.mask)\n",
    "\n",
    "# deepest data within each column\n",
    "topo = gr_dep.max(axis=0).compressed().reshape(1,-1)\n",
    "\n",
    "# interface positions between valid data\n",
    "gr_int = (gr_dep[1:] + gr_dep[:-1]) / 2\n",
    "m = gr_int.mask\n",
    "gr_int = gr_int.filled()\n",
    "\n",
    "# fill masks within each column with topo\n",
    "# add a millimetre for each successive masked value\n",
    "# so we don't have anything with exactly zero thickness\n",
    "gr_int[m] = (np.tile(topo, (gr_int.shape[0], 1)) + 1e-3 * np.cumsum(m, axis=0))[m]\n",
    "\n",
    "# prepend 0 to each column, append topo\n",
    "gr_int = np.concatenate((np.zeros((1, gr_int.shape[1])), gr_int, topo), axis=0)\n",
    "gr_lay = (gr_int[1:,:] + gr_int[:-1,:]) / 2\n",
    "\n",
    "# calculate thickness\n",
    "gr_h   = np.diff(gr_int, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert our temperature and salinity data from masked arrays to being defined on a full-depth grid of a generalised vertical coordinate, where masked values become vanished layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = sa_sect.mask\n",
    "bot_i = np.cumsum(~m, axis=0).max(axis=0) - 1\n",
    "\n",
    "# absolute salinity for remapping\n",
    "sa_bot = sa_sect[bot_i,np.arange(sa_sect.shape[1])].compressed().reshape(1,-1)\n",
    "sa_map = sa_sect.filled() # un-mask array\n",
    "sa_map[m] = np.tile(sa_bot, (sa_sect.shape[0], 1))[m]\n",
    "\n",
    "# conservative temperature\n",
    "ct_bot = ct_sect[bot_i,np.arange(ct_sect.shape[1])].compressed().reshape(1,-1)\n",
    "ct_map = ct_sect.filled()\n",
    "ct_map[m] = np.tile(ct_bot, (ct_sect.shape[0], 1))[m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go about actually generating the grid from the data in a couple of different ways:\n",
    "\n",
    "The first way is to use the Burchard and Beckers (2004) vertical grid diffusion with a coefficient representing the mixed layer region, and another representing the adjustment required to minimise local neutral density curvature.\n",
    "\n",
    "The second way is more along the lines of Hofmeister et al. (2010). This still involves a vertical grid diffusion, but we're not using any of the terms in this equation, so perhaps we should ignore it. Then we're left with the isopycnal (or in our case, neutral density difference) tendency term. Because this may lead to invalid layers, we follow up with a corrective step that's weighted in order to preserve resolution in the mixed layer.\n",
    "\n",
    "## Neutral Density Difference\n",
    "\n",
    "Using ghost cells, we can calculate the neutral density difference for every cell. Not sure what to do about vanished cells, but for the moment we'll just give them the same value as the first non-vanished cell above them in the same column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ndd(z_lay, sa_lay, ct_lay):\n",
    "    \"\"\"\n",
    "    Calculate neutral density difference (curvature) between\n",
    "    adjacent columns, given their (absolute) salinity, (conservative)\n",
    "    temperature and physical positions (or pressure).\n",
    "    \"\"\"\n",
    "    \n",
    "    # for dealing with edges, extend data with a ghost column\n",
    "    # layer positions with ghosts\n",
    "    #\"\"\"\n",
    "    lay_gst = np.concatenate([ z_lay[:,[1]],  z_lay,  z_lay[:,[-2]]], axis=1)\n",
    "    sa_gst  = np.concatenate([sa_lay[:,[1]], sa_lay, sa_lay[:,[-2]]], axis=1)\n",
    "    ct_gst  = np.concatenate([ct_lay[:,[1]], ct_lay, ct_lay[:,[-2]]], axis=1)\n",
    "    \"\"\"\n",
    "    lay_gst = np.concatenate([ z_lay[:,[0]],  z_lay,  z_lay[:,[0]]], axis=1)\n",
    "    sa_gst  = np.concatenate([sa_lay[:,[0]], sa_lay, sa_lay[:,[0]]], axis=1)\n",
    "    ct_gst  = np.concatenate([ct_lay[:,[0]], ct_lay, ct_lay[:,[0]]], axis=1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # neutral density curvature *at layers*\n",
    "    ndd = np.empty_like(z_lay)\n",
    "\n",
    "    # calculate for each (real) column\n",
    "    for i in range(sa_lay.shape[1]):\n",
    "        # calculate difference to column at left\n",
    "        sa_c =  sa_gst[:,[i,i+1]].mean(axis=1)\n",
    "        ct_c =  ct_gst[:,[i,i+1]].mean(axis=1)\n",
    "        z_c  = lay_gst[:,[i,i+1]].mean(axis=1)\n",
    "\n",
    "        r, a, b = gsw.rho_alpha_beta(sa_c, ct_c, z_c)\n",
    "        ndd_l = r * (b * (sa_gst[:,i+1] - sa_gst[:,i]) - a * (ct_gst[:,i+1] - ct_gst[:,i]))\n",
    "\n",
    "        # calculate difference to column at right\n",
    "        sa_c =  sa_gst[:,[i+2,i+1]].mean(axis=1)\n",
    "        ct_c =  ct_gst[:,[i+2,i+1]].mean(axis=1)\n",
    "        z_c  = lay_gst[:,[i+2,i+1]].mean(axis=1)\n",
    "\n",
    "        r, a, b = gsw.rho_alpha_beta(sa_c, ct_c, z_c)\n",
    "        ndd_r = r * (b * (sa_gst[:,i+2] - sa_gst[:,i+1]) - a * (ct_gst[:,i+2] - ct_gst[:,i+1]))\n",
    "\n",
    "        ndd[:,i] = ndd_r - ndd_l\n",
    "        \n",
    "    return ndd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ndd_int(z_int, sa_lay, ct_lay):\n",
    "    \"\"\"\n",
    "    Calculate neutral density difference (curvature) between\n",
    "    adjacent columns, given their (absolute) salinity, (conservative)\n",
    "    temperature and physical positions (or pressure).\n",
    "    \"\"\"\n",
    "    \n",
    "    sa_int = (sa_lay[1:,:] + sa_lay[:-1,:]) / 2\n",
    "    ct_int = (ct_lay[1:,:] + ct_lay[:-1,:]) / 2\n",
    "    \n",
    "    # for dealing with edges, extend data with a ghost column\n",
    "    # layer positions with ghosts\n",
    "    int_gst = np.concatenate([ z_int[:,[1]],  z_int,  z_int[:,[-2]]], axis=1)\n",
    "    sa_gst  = np.concatenate([sa_int[:,[1]], sa_int, sa_int[:,[-2]]], axis=1)\n",
    "    ct_gst  = np.concatenate([ct_int[:,[1]], ct_int, ct_int[:,[-2]]], axis=1)\n",
    "    \n",
    "    # neutral density curvature *at layers*\n",
    "    ndd = np.empty_like(z_int)\n",
    "\n",
    "    # calculate for each (real) column\n",
    "    for i in range(sa_int.shape[1]):\n",
    "        # calculate difference to column at left\n",
    "        sa_c =  sa_gst[:,[i,i+1]].mean(axis=1)\n",
    "        ct_c =  ct_gst[:,[i,i+1]].mean(axis=1)\n",
    "        z_c  = int_gst[:,[i,i+1]].mean(axis=1)\n",
    "\n",
    "        r, a, b = gsw.rho_alpha_beta(sa_c, ct_c, z_c)\n",
    "        ndd_l = r * (b * (sa_gst[:,i+1] - sa_gst[:,i]) - a * (ct_gst[:,i+1] - ct_gst[:,i]))\n",
    "\n",
    "        # calculate difference to column at right\n",
    "        sa_c =  sa_gst[:,[i+2,i+1]].mean(axis=1)\n",
    "        ct_c =  ct_gst[:,[i+2,i+1]].mean(axis=1)\n",
    "        z_c  = int_gst[:,[i+2,i+1]].mean(axis=1)\n",
    "\n",
    "        r, a, b = gsw.rho_alpha_beta(sa_c, ct_c, z_c)\n",
    "        ndd_r = r * (b * (sa_gst[:,i+2] - sa_gst[:,i+1]) - a * (ct_gst[:,i+2] - ct_gst[:,i+1]))\n",
    "\n",
    "        ndd[:,i] = ndd_r - ndd_l\n",
    "        \n",
    "    return ndd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def k_ndd_int(ndd_int):\n",
    "    \"\"\"\n",
    "    Calculate k_ndd on layers depending on values of\n",
    "    ndd at the surrounding interfaces.\n",
    "    If ndd is diverging at a layer, there is a finite value of k_ndd\n",
    "    otherwise, for converging ndd the diffusivity is zero.\n",
    "    \"\"\"\n",
    "    \n",
    "    # set ndd to zero at top and bottom boundaries\n",
    "    z = np.zeros((1, ndd_int.shape[1]))\n",
    "    ndd = np.concatenate((z, ndd_int, z), axis=0)\n",
    "    \n",
    "    # diverging -> positive k_ndd\n",
    "    # converging -> zero k_ndd\n",
    "    return np.maximum(ndd[1:,:] - ndd[:-1,:], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if we calcluated ndd on interfaces,\n",
    "# interpolate from interfaces to layers so we can define a diffusivity\n",
    "#ndd_lay = np.empty((ndd.shape[0] + 1, ndd.shape[1]))\n",
    "#for i in range(ndd.shape[1]):\n",
    "#    f            = interp1d(gr_int[1:-1,i], ndd[:,i], bounds_error=False, fill_value=\"extrapolate\")\n",
    "#    ndd_lay[:,i] = f(gr_lay[:,i])\n",
    "\n",
    "# otherwise, ndd is already in the right place\n",
    "ndd_lay = ndd(gr_lay, sa_map, ct_map)\n",
    "    \n",
    "# calculate ndd diffusivity coefficients by normalising by top-to-bottom potential density difference\n",
    "k_ndd = ndd_lay / (rhop_sect[bot_i,np.arange(rhop_sect.shape[1])] - rhop_sect[0,:])\n",
    "#k_ndd = np.maximum(k_ndd, 0)\n",
    "k_ndd = k_ndd**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at these neutral density differences `ndd`, and also normalise them by the top-to-bottom potential density difference from `rhop_sect`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = plt.subplot(211)\n",
    "plt.pcolormesh(lon_sect, gr_lay, ndd_lay)\n",
    "ax.invert_yaxis()\n",
    "plt.colorbar()\n",
    "\n",
    "# normalise by potential density difference in each column\n",
    "ax = plt.subplot(212)\n",
    "plt.pcolormesh(lon_sect, gr_lay, k_ndd)\n",
    "plt.colorbar()\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusing interfaces\n",
    "\n",
    "Now we can apply our diffusivity equation to interfaces, using the normalised `ndd` above as the grid diffusion coefficients. We start with interfaces at `gr_int`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def diffuse(z_int, k_ndd, c_ndd, c_surf, d_surf, dt):\n",
    "    \"\"\"\n",
    "    Use an implicit diffusivity equation to evolve the grid defined by z_int at timestep dt.\n",
    "    Neutral density difference diffusivity coefficient k_ndd is applied with weight c_ndd\n",
    "    (background diffusivity is then applied with weight (1 - c_ndd)).\n",
    "    \"\"\"\n",
    "    \n",
    "    # allocate new grid\n",
    "    z_next = np.empty_like(z_int)\n",
    "\n",
    "    # iterate over columns\n",
    "    for i in range(z_int.shape[1]):\n",
    "        # determine total grid coefficient from\n",
    "        # ndd term k_ndd\n",
    "        # background term 1/D, where D is local depth\n",
    "        # k_grid = D/tgrid * (c_ndd * k_ndd + c_b * k_b)\n",
    "        k_grid = (z_int[-1,i] * (c_ndd * k_ndd[:,i] + c_surf / (d_surf + z_lay[:,i])) \\\n",
    "                  + (1 - c_ndd - c_surf)) / 3600\n",
    "\n",
    "        I = z_int.shape[0] - 1 # number of layers\n",
    "        A = np.zeros((3, I+1)) # diffusion system coefficients\n",
    "\n",
    "        A[0,2:]     = -dt * I**2 * k_grid[1:]\n",
    "        A[2,:-2]    = -dt * I**2 * k_grid[:-1]\n",
    "        A[1,[0,-1]] = 1 # boundary conditions\n",
    "        A[1,1:-1]   = 1 + I**2 * dt * (k_grid[1:] + k_grid[:-1])\n",
    "\n",
    "        # solve tridiagonal system\n",
    "        z_next[:,i] = solve_banded((1, 1), A, z_int[:,i])\n",
    "        \n",
    "    return z_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gr_next = diffuse(gr_int, k_ndd, 0.3, 0.2, 50, 10)\n",
    "h_next = np.diff(gr_next, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = plt.subplot(121)\n",
    "plt.plot(gr_int)\n",
    "ax.invert_yaxis()\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "plt.plot(gr_next)\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = plt.subplot(211)\n",
    "plt.pcolormesh(gr_h)\n",
    "plt.colorbar()\n",
    "ax.invert_yaxis()\n",
    "\n",
    "ax = plt.subplot(212)\n",
    "plt.pcolormesh(h_next)\n",
    "plt.colorbar()\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remapping\n",
    "\n",
    "Now that we've obtained new interface positions `gr_next`, we can remap and see if we've improved the local neutral density curvature!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remap_cs = mom_remapping.Remapping_Cs()\n",
    "remap_cs.remapping_scheme = 4 # PQM_IH4IH3\n",
    "remap_cs.degree = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remap(h):\n",
    "    \"\"\"\n",
    "    Remap from original climatological grid according to h\n",
    "    \"\"\"\n",
    "\n",
    "    sa_remap = np.empty_like(sa_map)\n",
    "    ct_remap = np.empty_like(ct_map)\n",
    "\n",
    "    for i in range(sa_remap.shape[1]):\n",
    "        sa_remap[:,i] = mom_remapping.remapping_core_h(gr_h[:,i], sa_map[:,i], h[:,i], remap_cs)\n",
    "        ct_remap[:,i] = mom_remapping.remapping_core_h(gr_h[:,i], ct_map[:,i], h[:,i], remap_cs)\n",
    "        \n",
    "    return sa_remap, ct_remap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sa_remap, ct_remap = remap(h_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = plt.subplot(221)\n",
    "plt.pcolormesh(sa_map)\n",
    "ax.invert_yaxis()\n",
    "\n",
    "ax = plt.subplot(222)\n",
    "plt.pcolormesh(sa_remap)\n",
    "ax.invert_yaxis()\n",
    "\n",
    "ax = plt.subplot(223)\n",
    "plt.pcolormesh(ct_map)\n",
    "ax.invert_yaxis()\n",
    "\n",
    "ax = plt.subplot(224)\n",
    "plt.pcolormesh(ct_remap)\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative refinement\n",
    "\n",
    "Now we can just encapsulate the whole process in a loop over timesteps. Remapping is always done from the source data, not progressively, so we can try to preserve the original structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z_lay = gr_lay\n",
    "z_int = gr_int\n",
    "sa    = sa_map\n",
    "ct    = ct_map\n",
    "\n",
    "ndd_init = ndd(z_lay, sa, ct)\n",
    "rhop_init = np.empty_like(sa)\n",
    "for i in range(sa.shape[1]):\n",
    "    rhop_init[:,i] = gsw.rho(sa[:,i], ct[:,i], 2000)\n",
    "\n",
    "for k in range(10):\n",
    "    # use 0.5 as reference density difference\n",
    "    #k_ndd  = np.abs(ndd(z_lay, sa, ct) / 0.5)\n",
    "    k_ndd  = k_ndd_int(ndd_int(z_int[1:-1], sa, ct)) / 0.5\n",
    "    z_int  = diffuse(z_int, k_ndd, c_ndd=0.2, c_surf=0.4, d_surf=100, dt=10)\n",
    "    z_lay  = (z_int[1:,:] + z_int[:-1,:]) / 2\n",
    "    h      = np.diff(z_int, axis=0)\n",
    "    sa, ct = remap(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's see the final `ndd` state, and how it has changed from the initial state. It looks here as though we're actually increasing the neutral density curvature..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ndd_final = ndd(z_lay, sa, ct)\n",
    "\n",
    "ax = plt.subplot(211)\n",
    "plt.pcolormesh(ndd_final)\n",
    "ax.invert_yaxis()\n",
    "\n",
    "ax = plt.subplot(212)\n",
    "plt.pcolormesh(ndd_final - ndd_init)\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at layer thicknesses. Because `k_ndd` is quite discontinuous, depending on convergences and divergences, there are large changes in thickness within a column and within a layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.pcolormesh(h)\n",
    "plt.colorbar()\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rhop_final = np.empty_like(sa)\n",
    "for i in range(sa.shape[1]):\n",
    "    rhop_final[:,i] = gsw.rho(sa[:,i], ct[:,i], 2000)\n",
    "\n",
    "ax = plt.subplot(211)\n",
    "plt.pcolormesh(rhop_sect)\n",
    "ax.invert_yaxis()\n",
    "\n",
    "ax = plt.subplot(212)\n",
    "plt.pcolormesh(lon_sect, z_lay, rhop_final)\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = plt.subplot(211)\n",
    "plt.pcolormesh(rhop_final)\n",
    "ax.invert_yaxis()\n",
    "\n",
    "ax = plt.subplot(212)\n",
    "plt.pcolormesh(rhop_final - rhop_init)\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(z_lay, '*')\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(z_lay - gr_lay, '*');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
